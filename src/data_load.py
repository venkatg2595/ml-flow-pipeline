import os
import json
import pandas as pd
from google.cloud import bigquery
from google.oauth2 import service_account

# ----------------------------
# ‚úÖ Google Cloud Authentication
# ----------------------------

# Load credentials from environment variable
gcp_credentials = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")

if not gcp_credentials:
    raise ValueError("‚ùå Google Cloud credentials not found. Make sure GOOGLE_APPLICATION_CREDENTIALS is set.")

# Ensure the credentials file exists
if not os.path.exists(gcp_credentials):
    raise FileNotFoundError(f"‚ùå Google Cloud credentials file not found: {gcp_credentials}")

credentials = service_account.Credentials.from_service_account_file(gcp_credentials)
client = bigquery.Client(credentials=credentials)

# ----------------------------
# ‚úÖ Load Data from GCS
# ----------------------------

# Replace with your GCS bucket and file name
BUCKET_NAME = "mlops-bucket12"
FILE_NAME = "titanic.csv"
GCS_PATH = f"gs://{BUCKET_NAME}/{FILE_NAME}"

print(f"üìÇ Loading data from: {GCS_PATH}")

# Read CSV file from GCS
try:
    df = pd.read_csv(GCS_PATH)
    print("‚úÖ File loaded successfully.")
except Exception as e:
    print(f"‚ùå Error loading file: {e}")
    exit(1)

# Print available columns before processing
print("üîç Available Columns:", df.columns.tolist())

# ----------------------------
# ‚úÖ Preprocessing
# ----------------------------

df.fillna(df.mean(numeric_only=True), inplace=True)  # Fill missing values

# Ensure column exists before mapping
if 'sex' in df.columns:
    df['Sex'] = df['Sex'].map({'male': 0, 'female': 1})
else:
    print("‚ö†Ô∏è Column 'Sex' not found. Skipping mapping.")

# Ensure column exists before dropping
if 'embarked' in df.columns:
    df.drop(['embarked'], axis=1, inplace=True)
    print("‚úÖ Dropped column: 'embarked'")
else:
    print("‚ö†Ô∏è Column 'embarked' not found. Skipping drop operation.")

# Ensure target variable exists before splitting
if 'Survived' in df.columns:
    X = df.drop('Survived', axis=1)
    y = df['Survived']
else:
    print("‚ùå Column 'Survived' not found. Check dataset!")
    exit(1)

# Feature engineering (example)
if 'sibsp' in X.columns and 'parch' in X.columns:
    X['FamilySize'] = X['sibsp'] + X['parch'] + 1
    print("‚úÖ Created 'FamilySize' feature.")

# ----------------------------
# ‚úÖ Save Processed Data to BigQuery
# ----------------------------

DATASET_ID = "mlops_dataset"  # Replace with your dataset
TABLE_NAME = "titanic_data"
TABLE_ID = f"{client.project}.{DATASET_ID}.{TABLE_NAME}"

print(f"üì° Uploading to BigQuery table: {TABLE_ID}")

try:
    job = client.load_table_from_dataframe(df, TABLE_ID)
    job.result()  # Wait for job to complete
    print("‚úÖ Data successfully uploaded to BigQuery!")
except Exception as e:
    print(f"‚ùå Error uploading to BigQuery: {e}")
